<!DOCTYPE html>
<html>
    <head>
        <title>Deep Learning</title>
    </head>
    <body>
        <h1>Deep Learning</h1>
        <h2>History</h2>
        <p>IBM's DeepBlue machine defeated world chess champion Garry Kasparov in 1997. However, the chess world is rather small.</p>
        <p>Knowledge was first hard coded using formal language with logical inference rules. This was known as <b>knowledge based</b> approach.</p>
        <p>Then <b>Machine Learning</b> was developed to have the capacity to extract patterns from raw data, automating the need to actually hard code the knowledge base. 
            E.g. We could use <b>Logistic Regression</b> to determine whether we should use cesarean delivery or not, <b>naive Bayes</b> to classify mail as spam or not, etc.
        </p>
        <p>With this the <b>Representation</b> of data also hold a significance. The data which is used to train the model using Machine learning alogirthms have attributes called <b>features</b></p>
        <p>Finding proper representation of data is sometimes time consuming, so people came with the idea of using ML to find good set of features. This is known as <b>Representation Learning</b>. 
            <br />E.g. <b>Autoencoders</b> which consists of <b>encoders</b> and <b>decoders </b>are used to find good representation of data.    
        </p>
        <p><b>Factors of Variation</b> are the features which explains the data and influences the output.</p>
    </body>
</html>